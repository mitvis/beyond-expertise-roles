{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "needs_list = ['G1', 'G2', 'O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', \n",
    "         'T1', 'T2', 'T3', 'T4', 'T5']\n",
    "for col in needs_list: \n",
    "    df[col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(df.codes):\n",
    "    row_codes = row.split(';')\n",
    "    for need_string in row_codes: \n",
    "        need_string_split = need_string.split(\":\")\n",
    "        if len(need_string_split) > 1: \n",
    "            label, snippet = need_string_split\n",
    "        else: \n",
    "            label = need_string\n",
    "            snippet = \"general\"\n",
    "        \n",
    "        df.loc[i, label.strip()] = snippet.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('codes', axis=1, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakeholders = ['ML-Formal', 'ML-Instrumental', 'ML-Personal', 'Domain-Formal',\n",
    "       'Domain-Instrumental', 'Domain-Personal', 'Milieu-Formal',\n",
    "       'Milieu-Instrumental', 'Milieu-Personal']\n",
    "goals = ['G1', 'G2']\n",
    "objectives = [ 'O1', 'O2', 'O3',\n",
    "       'O4', 'O5', 'O6', 'O7']\n",
    "tasks = ['T1', 'T2', 'T3', 'T4', 'T5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_stakeholders = df[stakeholders].isnull().apply(lambda x: all(x), axis=1) \n",
    "df['Stakeholders-Not-Specified'] = np.nan\n",
    "df.loc[all_nan_stakeholders, 'Stakeholders-Not-Specified'] = 'general'\n",
    "stakeholders.append('Stakeholders-Not-Specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_goals = df[goals].isnull().apply(lambda x: all(x), axis=1) \n",
    "df['Goal-Not-Specified'] = np.nan\n",
    "df.loc[all_nan_goals, 'Goal-Not-Specified'] = 'general'\n",
    "goals.append('Goal-Not-Specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nan_objectives = df[objectives].isnull().apply(lambda x: all(x), axis=1) \n",
    "df['Obj-Not-Specified'] = np.nan\n",
    "df.loc[all_nan_objectives, 'Obj-Not-Specified'] = 'general'\n",
    "objectives.append('Obj-Not-Specified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data = []\n",
    "for i in range(df.shape[0]):\n",
    "    row = df.loc[i]\n",
    "    keys = row.dropna().keys()\n",
    "    stakeholders_in_row = [k for k in keys if k in stakeholders]\n",
    "    goals_in_row = [k for k in keys if k in goals]\n",
    "    objectives_in_row = [k for k in keys if k in objectives]\n",
    "    tasks_in_row = [k for k in keys if k in tasks]\n",
    "    for s in stakeholders_in_row: \n",
    "        for g in goals_in_row: \n",
    "            link_data.append({\n",
    "                'source': s,\n",
    "                'target': g, \n",
    "                'level': 0,\n",
    "                'paper': i\n",
    "            })\n",
    "            \n",
    "    for g in goals_in_row: \n",
    "        for o in objectives_in_row: \n",
    "            link_data.append({\n",
    "                'source': g,\n",
    "                'target': o,\n",
    "                'level': 1,\n",
    "                'paper': i\n",
    "            })\n",
    "            \n",
    "    for o in objectives_in_row: \n",
    "        for t in tasks_in_row: \n",
    "            link_data.append({\n",
    "                'source': o,\n",
    "                'target': t,\n",
    "                'level': 2, \n",
    "                'paper': i\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = pd.DataFrame(link_data).groupby(['source', 'target'])\n",
    "paper_list = gb['paper'].apply(list)\n",
    "level = gb['level'].mean()\n",
    "count = gb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.Index(gb.groups.keys()).set_names(['source', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.DataFrame(index=idx, data={'paper_list': paper_list, 'level': level, 'count': count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df_json = agg_df.reset_index().to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_link_data = json.loads(agg_df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for s in stakeholders: \n",
    "    nodes.append({'name': s, 'category': 'knowledge', \n",
    "                  'paper_list': list(df[~pd.isnull(df[s])].index), \n",
    "                  'snippets': {key: value for key, value in df[~pd.isnull(df[s])][s].to_dict().items() if value != 'general'}\n",
    "})\n",
    "for g in goals: \n",
    "    nodes.append({'name': g, 'category': 'goals', \n",
    "                  'paper_list': list(df[~pd.isnull(df[g])].index),\n",
    "                  'snippets': {key: value for key, value in df[~pd.isnull(df[g])][g].to_dict().items() if value != 'general'}\n",
    "})\n",
    "for o in objectives: \n",
    "    nodes.append({'name': o, 'category': 'objectives', \n",
    "                  'paper_list': list(df[~pd.isnull(df[o])].index),\n",
    "                  'snippets': {key: value for key, value in df[~pd.isnull(df[o])][o].to_dict().items() if value != 'general'}\n",
    "})\n",
    "for t in tasks: \n",
    "    nodes.append({'name': t, 'category': 'tasks', \n",
    "                  'paper_list': list(df[~pd.isnull(df[t])].index),\n",
    "                  'snippets': {key: value for key, value in df[~pd.isnull(df[t])][t].to_dict().items() if value != 'general'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ML-Formal',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [3, 4, 5, 12, 13, 14, 16, 17, 20, 33, 37, 38, 41, 45, 53, 54],\n",
       "  'snippets': {3: 'AI experts are machine learning scientists and engineers who design machine learning algorithms',\n",
       "   4: 'Experts: This group of users are experienced in deep learning, and they wish to have a brief idea of the research field in deep learning visualization.',\n",
       "   5: 'model builders',\n",
       "   12: 'Model developers',\n",
       "   13: ' ML Engineers',\n",
       "   14: 'model developers',\n",
       "   16: 'The very experts who understand decision-making models',\n",
       "   17: 'DNN engineers ',\n",
       "   20: 'Creators',\n",
       "   33: 'machine learning researchers',\n",
       "   37: 'Model-developers and builders',\n",
       "   38: 'ML experts, which are people capable of building, training and testing machine learning models with different datasets from different domains.',\n",
       "   41: 'Developers and AI researchers: investigators in AI, software developers, or data analysts who create the AI system.  KN: * Lay users: \"with no technical background, would not be interested at all about these type of explanation\"',\n",
       "   45: 'model developers',\n",
       "   53: 'those with some existing ML experience such as CS students, data scientists, product managers, and ML practitioners.',\n",
       "   54: 'model analyst'}},\n",
       " {'name': 'ML-Instrumental',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [0,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   31,\n",
       "   34,\n",
       "   35,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   52,\n",
       "   53],\n",
       "  'snippets': {0: 'medical experts’ increasing familiarity with [computer-aided diagnosis] systems; \"user expectations of an AI’s performance and metrics can be strongly anchored to their prior experience with detection-based models (section 5.1.1); as seen in our study, this can sometimes yield unrealistic expectations.\"',\n",
       "   3: 'Data experts include data scientists and domain experts who use machine learning for analysis, decision-making, or research',\n",
       "   4: 'Practitioners: Users who already have some experience and understand what deep learning is. They want to get the intuition of how the architectures look like when dealing with deep learning models.  \"Developers: Users who design and improve deep learning models, find it difficult to debug during training process and tune the parameters.\"',\n",
       "   5: 'optimization expertise',\n",
       "   13: 'data scientists',\n",
       "   14: 'model users',\n",
       "   15: 'broadly engage the greater machine learning community',\n",
       "   17: 'DNN developers',\n",
       "   19: 'industry practitioners, \"data scientists or machine learning engineers, 2 identified themselves as software engineers whose goal is to build infrastructure related to model interpretability\"',\n",
       "   20: 'Implementers',\n",
       "   21: 'Data scientists and Machine Learning practitioners \"study data scientists’ use of two existing interpretability tools\"',\n",
       "   24: 'data scientist users, who may not be experts in explainability, as well as algorithm developers',\n",
       "   27: 'ML engineer',\n",
       "   28: 'primarily graduate students with backgrounds from design, biomedical engineering, and education. Participants had varying levels of experience with AI and machine learning, ranging from 0–5 years of experience.',\n",
       "   29: 'industry practitioners to create explainable AI products',\n",
       "   31: 'Several had helped to develop individual components of the system. Others were managers or researchers contributing to individual aspects of the system',\n",
       "   34: 'An engineering perspective might naturally lead one to suspect that more information should lead to a better outcome',\n",
       "   35: 'data scientists',\n",
       "   37: 'These are users who may have some technical background but are neural network novices   \". Common tasks include using well-known neural network architectures for developing domain specific applications, training smaller-scale models, and downloading pre-trained model weights online to use as a starting point. This group of users also include machine learning artists who use models to enable and showcase new forms of artistic expression.',\n",
       "   38: 'Non-ML-experts ...[who] use ML tools to perform tasks..',\n",
       "   39: 'Clinicians repeatedly identified that knowing the subset of features deriving the model outcome, is crucial',\n",
       "   40: ' \"data scientist\"; \"end users of ML and AI systems\"',\n",
       "   52: 'autonomous robot developers',\n",
       "   53: 'practitioners'}},\n",
       " {'name': 'ML-Personal',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [4, 16, 37],\n",
       "  'snippets': {4: 'It is essential for experts and developers to quickly build the intuition of how the network looks like',\n",
       "   16: \"the researchers' intuition of what constitutes a ‘good’ explanation.\",\n",
       "   37: 'well-developed intuition...mastery over models... vary hyperparameters'}},\n",
       " {'name': 'Domain-Formal',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [0,\n",
       "   1,\n",
       "   5,\n",
       "   6,\n",
       "   10,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   18,\n",
       "   20,\n",
       "   22,\n",
       "   33,\n",
       "   34,\n",
       "   39,\n",
       "   41,\n",
       "   45,\n",
       "   47,\n",
       "   51,\n",
       "   52,\n",
       "   54],\n",
       "  'snippets': {0: 'pathologists',\n",
       "   1: 'Each participant is a student enrolled in a law school,  \"Each participant acknowledged having in-depth knowledge (16 participants) or at least some familiarity (31 participants) with the bail decision making process. \"',\n",
       "   5: 'agronomic engineers',\n",
       "   6: 'AI inferences to help human experts make better decisions, e.g., with respect to medical diagnoses, recidivism prediction, and credit assessment',\n",
       "   10: 'doctors, judges',\n",
       "   12: 'scientists',\n",
       "   13: 'regulators',\n",
       "   14: 'biologist (model user)',\n",
       "   18: 'HR managers who produce expert estimates',\n",
       "   20: 'business logic for this decision will have been decided on by more senior employees at the lender; \"Examiners\" - \"tasked with compliance/safety-testing, auditing, or forensically investigating a system\"',\n",
       "   22: 'end users—doctors',\n",
       "   33: 'doctors performing diagnoses',\n",
       "   34: 'the background of an economist or multi-agent game theorist helps to realize that more information empowers the agents to optimize their own agendas more efficiently',\n",
       "   39: 'clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department), Provide a level of transparency that allows users to validate model outputs with domain knowledge.\"',\n",
       "   41: 'Domain experts: specialists in the area of expertise where the decisions made by the system belong to. For example: physicists or lawyers.',\n",
       "   45: 'doctors',\n",
       "   47: 'doctors',\n",
       "   51: 'energy data operators',\n",
       "   52: 'autonomous robot developers',\n",
       "   54: 'medical expert'}},\n",
       " {'name': 'Domain-Instrumental',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [0,\n",
       "   5,\n",
       "   7,\n",
       "   12,\n",
       "   14,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   24,\n",
       "   27,\n",
       "   32,\n",
       "   33,\n",
       "   36,\n",
       "   39,\n",
       "   42,\n",
       "   43,\n",
       "   44],\n",
       "  'snippets': {0: 'clinicians',\n",
       "   5: 'only specialists in part of the underlying process',\n",
       "   7: 'Internal financial auditors',\n",
       "   12: 'scientists increasingly adopt ML for optimizing and producing scientific outcomes',\n",
       "   14: 'model novices (These are non-experts in ML, interested in understanding ML concepts and getting to know more about applying ML models, e.g., for specific domains.)',\n",
       "   18: 'Direct reports who use the estimates',\n",
       "   19: 'Various domain users listed in Table 1.',\n",
       "   20: 'Operators',\n",
       "   24: 'specialist with deep knowledge of the circumstances for employee retention',\n",
       "   27: 'doctor [using a disease recognition system]',\n",
       "   32: 'with domain knowledge, an expert can inspect the produced explanation visualizations to verify the result qualitatively',\n",
       "   33: 'clinician',\n",
       "   36: 'end users of systems built using ML models',\n",
       "   39: 'clinicians',\n",
       "   42: 'According to Model 6, higher technical literacy was associated with higher levels of self-reported understanding (Coef. = 0.59, p<0.001).',\n",
       "   43: 'participants of a MOOC course doing peer grading',\n",
       "   44: 'learners of sign language'}},\n",
       " {'name': 'Domain-Personal',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [9, 16, 20, 21, 22, 27, 29, 34, 36, 39, 40, 41],\n",
       "  'snippets': {9: 'operators might be more interested how well the system’s conceptual model fits their mental model ',\n",
       "   16: 'they will construct a better mental model of the system and be able to generalise its behaviour (effectively learning its model)',\n",
       "   20: 'decision-subjects, \"data-subjects\"',\n",
       "   21: 'Without accurate mental models, social factors can rationalize suspicious observations [about explanations]',\n",
       "   22: 'shared decision-making [...] meaningful dialogue between healthcare providers and patients (presumably for the patient to share lived experience)',\n",
       "   27: 'patient using a disease recognition system ',\n",
       "   29: 'lay users, who may not have deep technical understanding of AI, but hold preconception of what constitutes useful explanations for decisions',\n",
       "   34: 'patient, \"client\"',\n",
       "   36: 'trust in the model is significantly affected by its observed accuracy regardless of its stated accuracy',\n",
       "   39: 'critical factor for continued usage was whether the tool was repeatedly successful in prognosticating their patient’s condition in their personal experience.',\n",
       "   40: 'a lay person affected by an automated decision likely has his or her own intuitive notion of what outcomes seem fair or just. It is easy to imagine an affected individual dismissing an outcome',\n",
       "   41: 'Lay users: the final recipients of the decisions. For example: a person accepted or rejected on a loan demand, or a patient that has been diagnosed  KN: Not sure if they mention any knowledge status of lay users, although it would be strange to include AI researchers and domain experts, and not lay users just because no explanation was provided for the lay users.  HS: yeah, I generally put users described as \"decision subjects\" under milieu-personal in the few I coded.  '}},\n",
       " {'name': 'Milieu-Formal',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [17, 22, 35, 53],\n",
       "  'snippets': {17: 'ethicists',\n",
       "   22: 'bodies such as institutional review boards, ethics review committees, and health technology assessment organizations',\n",
       "   35: 'require understanding requirements arising from social contexts other than just from usability or human cognitive psychology',\n",
       "   53: 'A group of MIT students working on a project for the Foundations of Information Policy course made extensive use of WIT'}},\n",
       " {'name': 'Milieu-Instrumental',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [0, 3, 17, 19, 20, 29, 37, 39, 40],\n",
       "  'snippets': {0: '“More variation is better...Covering from community hospital small groups, to academic medical centers, it’s more representative.” (P16)',\n",
       "   3: 'AI novices refer to end-users who use AI products in daily life',\n",
       "   17: 'Stakeholders are often interested in the ethical and legal concerns raised in any phase of the process.',\n",
       "   19: 'product managers',\n",
       "   20: 'Examiners',\n",
       "   29: 'UX and design practitioners, \"regulators\"',\n",
       "   37: 'These are individuals who typically have no prior knowledge about deep learning, and may or may not have a technical background. Much of the research targeted at this group is for educational purposes, trying to explain what a neural network is and how it works at a high-level, sometimes without revealing deep learning is present. These group also includes people who simply use AI-powered devices and consumer applications.  \"These group also includes people who simply use AI-powered devices and consumer applications.\"',\n",
       "   39: '[Interview] settings were chosen because both departments are areas where we foresee implementation of ML tools to support clinical care. Additionally, both departments have current experience working with either early warning/alert systems or opaque, non-ML decision support tools',\n",
       "   40: 'When we shared the findings from this technology study with others in government and industry, we saw that today’s XAI tools do not fully capture the types of explanations that many people want.'}},\n",
       " {'name': 'Milieu-Personal',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [7, 10, 11, 16, 17, 18, 24, 27, 38, 39, 48, 49, 56],\n",
       "  'snippets': {7: 'what type of model behavior might be experienced by different cultural, demographic or phenotypic groups.',\n",
       "   10: 'Turkers doing sentiment analysis, \"followed my own discretion\"',\n",
       "   11: 'we see how a range of actors must make sense of an AI output – yet each of these actors bring their own points-of-view and own priorities, which can sometimes be in conflict. ',\n",
       "   16: 'people employ certain biases and social expectations when they generate and evaluate explanation, \"the social behaviour of others in physical environments\"',\n",
       "   17: 'Owner, end user, data subject',\n",
       "   18: 'Subject already determines interruptibility which the paper is trying to enhance using an intelligent system',\n",
       "   24: 'loan applicant',\n",
       "   27: 'Decision Information [...] general knowledge or experiences',\n",
       "   38: 'non-experts in context-agnostic contexts, discussing privacy and personal data',\n",
       "   39: 'Many clinicians noted that “alarm or click fatigue” (indicating the annoyance with repeating response prompts through the EHR system) (Embi and Leonard, 2012) is a significant concern that may be worsened by prediction tools. This issue is ubiqitous across healthcare contexts and requires careful consideration',\n",
       "   48: 'recommendation system users',\n",
       "   49: 'news recommender users',\n",
       "   56: 'affected decision subjects / loan applicants'}},\n",
       " {'name': 'Stakeholders-Not-Specified',\n",
       "  'category': 'knowledge',\n",
       "  'paper_list': [2, 8, 23, 25, 26, 30, 46, 50, 55],\n",
       "  'snippets': {}},\n",
       " {'name': 'G1',\n",
       "  'category': 'goals',\n",
       "  'paper_list': [0, 2, 6, 12, 15, 25, 31, 33, 34, 35, 36, 40, 42, 46, 50, 53],\n",
       "  'snippets': {0: '\"global transparency questions could be key to forming an accurate initial impression of an ML-based system\"',\n",
       "   2: '\"the ultimate goal is for people to understand machine models\"',\n",
       "   12: '\"to attain scientific outcomes with ML one wants an understanding\"',\n",
       "   31: '\"need to understand the agent’s behavior and responses enough to participate in the mixed-initiative execution process\"',\n",
       "   40: '\"they wanted better tools to determine if their code was executing properly, to confirm that code was doing what they thought it was doing, and to help them build intuition about how models worked.\"',\n",
       "   42: 'help users and other stakeholders understand the “algorithmic decision model”',\n",
       "   46: 'explain how the system works',\n",
       "   53: '\"medium for model understanding\"'}},\n",
       " {'name': 'G2',\n",
       "  'category': 'goals',\n",
       "  'paper_list': [1,\n",
       "   3,\n",
       "   8,\n",
       "   10,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   19,\n",
       "   20,\n",
       "   22,\n",
       "   23,\n",
       "   25,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   35,\n",
       "   36,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   42,\n",
       "   43,\n",
       "   46,\n",
       "   50,\n",
       "   54],\n",
       "  'snippets': {3: '\"User Trust and Reliance\"',\n",
       "   8: '\"To increase trust in such systems by human users\"',\n",
       "   10: 'build appropriate trust',\n",
       "   14: 'trust-building steering mechanism',\n",
       "   16: '\"to understand and therefore trust the intelligent agents\"',\n",
       "   19: '\"often aimed at building trust \"',\n",
       "   22: '\"lack of transparency can preclude the mechanistic interpretation of MLm-based assessments and, in turn, reduce their trustworthiness\"',\n",
       "   23: 'trust, increase user confidence in system',\n",
       "   25: '\"Improves trust\", definition of \"trustability\"',\n",
       "   28: '\"the goal of explainable interfaces should be instilling in users the right amount of trust\"',\n",
       "   29: '\"Explanation is often embraced as a cure for \\'black box\\' models to gain user trust and adoption\"',\n",
       "   31: '\"need to trust the reasoning and actions performed by the agent\"',\n",
       "   39: '\"Building trust between clinicians and ML models\"',\n",
       "   43: 'Trust is a key concern in the design of technology, as it affects the initial adoption and continued use of technologies',\n",
       "   50: '\"the questions assessed system understanding [...] as well as capability and benevolence, key dimensions of trust\"',\n",
       "   54: 'ensure that ML models reflect our values'}},\n",
       " {'name': 'Goal-Not-Specified',\n",
       "  'category': 'goals',\n",
       "  'paper_list': [4,\n",
       "   5,\n",
       "   7,\n",
       "   9,\n",
       "   11,\n",
       "   13,\n",
       "   17,\n",
       "   18,\n",
       "   21,\n",
       "   24,\n",
       "   26,\n",
       "   27,\n",
       "   32,\n",
       "   37,\n",
       "   41,\n",
       "   44,\n",
       "   45,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   51,\n",
       "   52,\n",
       "   55,\n",
       "   56],\n",
       "  'snippets': {}},\n",
       " {'name': 'O1',\n",
       "  'category': 'objectives',\n",
       "  'paper_list': [0,\n",
       "   4,\n",
       "   5,\n",
       "   8,\n",
       "   10,\n",
       "   12,\n",
       "   14,\n",
       "   16,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   28,\n",
       "   34,\n",
       "   35,\n",
       "   37,\n",
       "   39,\n",
       "   48,\n",
       "   50],\n",
       "  'snippets': {0: '\"struggled to determine the extent to which its diagnostic process could be similar to or different from their own\"',\n",
       "   4: '\"While it is difficult for users to comprehend the results of representation learning, analysis of the complicated and superimposed deep neural networks can be very challenging\"',\n",
       "   5: 'Explaining findings through storytelling to collaborators',\n",
       "   8: '\"It enables the user to consider contrastive explanations and counterfactual analyses, such as why one decision was made instead of another\"',\n",
       "   14: '\"enable a reasoned justification of the user’s decision-making\"',\n",
       "   16: '\"For explanation, if the goal of an explanatory agent is to provide the most likely causes of an event, then these three criteria can be used to prioritise among the most likely events. However, if the goal of an explanatory agent is to generate trust between itself and its human observers, these criteria should be considered as first-class criteria in explanation generation beside or even above likelihood.\"',\n",
       "   19: '\"explanations or justifications for model decisions being most often requested\"',\n",
       "   20: '\" After the event, this [military ML] order may be scrutinized at a tribunal.\"',\n",
       "   21: '\"[using interpretability visualizations to] uncover [...] missing values in a dataset that have been filled in incorrectly\", reasoning about data outputs',\n",
       "   24: '\"validating whether recommendations given by the model for different loan applications are justified\"',\n",
       "   26: '\"provides the required information to justify result\"',\n",
       "   34: '\"the ability to audit a prediction or decision trail in detail, particularly if something goes wrong\"',\n",
       "   35: '\"people needed to be able to understand what was being sensed and which actions were being taken based on that information\"',\n",
       "   37: 'explain decisions made by medical imaging models',\n",
       "   39: '\"viewed explainability as a means of justifying their clinical decision-making (for instance, to patients and colleagues)\"',\n",
       "   48: '\"what they do (e.g. how AGSs generate advice)\"',\n",
       "   50: 'give users insight into why the system recognized or did not recognize their drawing'}},\n",
       " {'name': 'O2',\n",
       "  'category': 'objectives',\n",
       "  'paper_list': [18, 23, 24, 28, 29, 33, 39, 41, 44, 45, 46, 47, 56],\n",
       "  'snippets': {24: '\"know about a few factors that could be changed to improve their profile for possible approval in the future\"',\n",
       "   28: '\"were asked their own [domain-related] decision [using the XAI or not]\"',\n",
       "   29: '“how the output impacts other system components\"',\n",
       "   33: '\"such as identifying errors in a safety-oriented task\"',\n",
       "   39: '\"facilitate trust in the model as well as directing use in specific patient populations and determining parameters guiding appropriate use\"',\n",
       "   41: 'right to an explanation that has consequences',\n",
       "   44: 'understand how to correct actions based on model feedback',\n",
       "   45: 'assist a human decision-maker, such as a doctor',\n",
       "   46: 'help make decisions better and faster, persuade users to try or buy',\n",
       "   47: '\"diagnosis and treatment purposes in their daily work practice\"',\n",
       "   56: 'understand how to get the desired outcome'}},\n",
       " {'name': 'O3',\n",
       "  'category': 'objectives',\n",
       "  'paper_list': [3,\n",
       "   4,\n",
       "   9,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   17,\n",
       "   19,\n",
       "   26,\n",
       "   29,\n",
       "   32,\n",
       "   34,\n",
       "   37,\n",
       "   38,\n",
       "   41,\n",
       "   45,\n",
       "   46,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55],\n",
       "  'snippets': {3: '\"help data experts to tune machine learning parameters for their specific data\", model debugging',\n",
       "   4: '\"optimizing the model of specific tasks, accelerating the training, or fine-tuning the parameters\"',\n",
       "   14: 'model refinement',\n",
       "   19: '\"identify issues with a model and devise ways to fix it\"',\n",
       "   26: '\"A model that can be explained and understood is one that can be more easily improved.\"',\n",
       "   29: '\"improve AI performance\"',\n",
       "   32: '\"To debug and optimize time series prediction models in diverse tasks, not only understanding is essential but also that the XAI explanation is correct itself\"',\n",
       "   34: '\"For a developer, to understand how their system is working, aiming to debug or improve it\"',\n",
       "   37: 'debugging',\n",
       "   38: '\"using explanations to improve an aspect, a part of the system, among others\"',\n",
       "   41: 'improvement',\n",
       "   45: 'guide model or training data improvements',\n",
       "   46: 'debugging',\n",
       "   52: 'development and debugging of agents',\n",
       "   53: 'debugging',\n",
       "   54: 'designing, developing, and debugging models',\n",
       "   55: 'debugging/imporvement'}},\n",
       " {'name': 'O4',\n",
       "  'category': 'objectives',\n",
       "  'paper_list': [0, 9, 11, 20, 30, 34, 56],\n",
       "  'snippets': {0: '“It’s hard for me to know how much to trust when I see things that I don’t completely agree with.”',\n",
       "   9: '\"present an incontestable subset of reasons to the bank employee\"',\n",
       "   11: 'contest AI recommendation about assisted living',\n",
       "   20: '\"idea of contestability\"',\n",
       "   30: '\"contest this premise in the hope of overturning the decision\"',\n",
       "   56: 'contest the decision if the desired outcome is discriminatory'}},\n",
       " {'name': 'O5',\n",
       "  'category': 'objectives',\n",
       "  'paper_list': [7,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   17,\n",
       "   20,\n",
       "   22,\n",
       "   26,\n",
       "   30,\n",
       "   33,\n",
       "   34,\n",
       "   38,\n",
       "   40,\n",
       "   41,\n",
       "   49,\n",
       "   55],\n",
       "  'snippets': {7: '\"frameworks through which independent audits can demonstrate adherence to standards\"',\n",
       "   15: '\"With impending regulations like the European Union’s \\'Right to Explanation\\'\"',\n",
       "   20: 'forensics, \"aid in their auditing or forensic investigations of ecosystems. They may focus on decision-subjects and data-subjects, ensuring that the system is compliant with their rights\"',\n",
       "   22: '\"Some degree of explainability may also be required to justify the clinical validation of MLm in prospective studies and randomized clinical trial\"',\n",
       "   26: '\"compliance with legislation, for instance the \\'right to explanation\\'\"',\n",
       "   33: 'GDPR will \"require algorithms that make decisions based on user-level predictors\"',\n",
       "   34: '\"To facilitate monitoring and testing for safety standards\"',\n",
       "   38: 'GDPR',\n",
       "   40: '\"Explainable AI as an important means of auditing models\"',\n",
       "   41: 'compliance with legislation',\n",
       "   49: 'compliance w/ GDPR',\n",
       "   55: 'compliance with legislation'}},\n",
       " {'name': 'O6',\n",
       "  'category': 'objectives',\n",
       "  'paper_list': [3, 9, 20, 24, 30, 48],\n",
       "  'snippets': {3: '\"Machine learning explanations can disclose to end-users what user data is being used in algorithmic decision-making\", bias mitigation, privacy awareness',\n",
       "   20: '\"Data-subjects may have moral concerns about how their data is being used to make decisions about other people\"',\n",
       "   48: 'why certain data is collected'}},\n",
       " {'name': 'O7',\n",
       "  'category': 'objectives',\n",
       "  'paper_list': [4, 19, 26, 27, 30, 33, 41, 44, 51, 55],\n",
       "  'snippets': {4: 'learning about ML, \"They want to get the intuition of how the architectures look like when dealing with deep learning models\"',\n",
       "   19: '\"as a vehicle to generate insights about the phenomena described by the data\"',\n",
       "   26: '\"Asking for explanations is a helpful tool to learn new facts, to gather information and thus to gain knowledge\"',\n",
       "   27: '\"Users were supposed to learn to solve the task, ie. image classification based on the explanations provided\"',\n",
       "   30: '\"uncover causal structure in observational data\"',\n",
       "   33: '\"The human’s goal is to gain knowledge\"',\n",
       "   41: 'learn from the system',\n",
       "   44: 'learn about sign language and how to do it correctly',\n",
       "   51: 'learn design strategies based on cluster commonalities \\x07\\x08\\x15',\n",
       "   55: 'learning game strategy (Go)'}},\n",
       " {'name': 'Obj-Not-Specified',\n",
       "  'category': 'objectives',\n",
       "  'paper_list': [1, 2, 6, 31, 36, 42, 43],\n",
       "  'snippets': {}},\n",
       " {'name': 'T1',\n",
       "  'category': 'tasks',\n",
       "  'paper_list': [5, 10, 12, 29, 36],\n",
       "  'snippets': {5: '\"Identify and explain an outlier\"',\n",
       "   10: '\"may have learned to rely on [the explanations] less\"',\n",
       "   12: '\"to ensure the scientific value of the outcome\"',\n",
       "   29: '\"to assess the AI’s judgment to make an informed decision\"',\n",
       "   36: '\"after observing a model’s accuracy in practice, people are more likely to increase their trust in the model if the model’s observed accuracy is higher than their own accuracy\"'}},\n",
       " {'name': 'T2',\n",
       "  'category': 'tasks',\n",
       "  'paper_list': [7, 11, 25, 26, 33, 41, 46, 55],\n",
       "  'snippets': {7: '\"it is incumbent on producers of artificial intelligence systems to anticipate ethics-related failures before launch\"',\n",
       "   11: 'bias detection, \"consider the experimental system’s predictions with their established ways of monitoring ADLs\"',\n",
       "   25: '\"Learning the model behavior using XAI techniques for different input data distributions could improve our understanding of the skewness and biases in the input data\"',\n",
       "   26: '\"provides greater visibility over unknown vulnerabilities and flaws\"',\n",
       "   33: '\"there might be biases that we did not consider a priori\"',\n",
       "   41: 'verification to detect bias',\n",
       "   46: 'detect mistakes',\n",
       "   55: 'verification of the system for bias/mistakes'}},\n",
       " {'name': 'T3',\n",
       "  'category': 'tasks',\n",
       "  'paper_list': [6, 10, 19, 29, 39],\n",
       "  'snippets': {6: 'understanding model error from predictions',\n",
       "   10: '\"know when to trust the AI’s suggestion and when to be skeptical\"',\n",
       "   19: '\"need for model builders to gain confidence in the reliability and validity of models they build\"',\n",
       "   29: '\"beware of the system’s limitations\"',\n",
       "   39: '\"clarity around why the model under-performs\"'}},\n",
       " {'name': 'T4',\n",
       "  'category': 'tasks',\n",
       "  'paper_list': [7, 12, 18, 31, 42, 48],\n",
       "  'snippets': {7: '\"data entanglement\"',\n",
       "   12: '\"the user can be informed when the scheme is being used for systems for which it is not suited\"',\n",
       "   18: 'understand \"what the system was sensing in order to make its inferences\"',\n",
       "   31: '\"knowing what resources were being used to provide answers\"',\n",
       "   42: 'see what attributes cause the algorithm’s action',\n",
       "   48: '\"What/how data is collected\"'}},\n",
       " {'name': 'T5',\n",
       "  'category': 'tasks',\n",
       "  'paper_list': [9, 13, 17, 19, 29, 30, 35, 42, 50, 53, 56],\n",
       "  'snippets': {9: '\"interested in the factors influencing their individual decision\"',\n",
       "   13: '\"how drift in feature distributions would impact model outcomes\"',\n",
       "   17: '\"did the factor \\'race\\' influence the outcome of the system”',\n",
       "   19: '\"understand the mechanism by which a model makes predictions\"',\n",
       "   29: '\"inspect how the output changes with instance changes\"',\n",
       "   30: '\"by probing the patterns that the model has extracted, we can convey additional information to a human decision maker\"',\n",
       "   35: '\"how feedforward can help people understand and predict what is going to happen\"',\n",
       "   42: 'the user can predict how changes in the situation can lead to alternative algorithm predictions',\n",
       "   50: 'understanding what about the drawing changes the prediction',\n",
       "   53: 'explore counterfactuals and how changes to data points affect predictions',\n",
       "   56: 'understand the influence of different factors to see what will change the outcome'}}]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_link_data = {'nodes': nodes, 'links': link_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('node_link_data.json', 'w') as fp:\n",
    "    json.dump(node_link_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibData = df[['author', 'year', 'title']].reset_index().to_json(orient='records')\n",
    "bibData = json.loads(bibData)\n",
    "with open('bib.json', 'w') as fp:\n",
    "    json.dump(bibData, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
